# DVBench: DeFi Vulnerability Finding Benchmark

A benchmark for evaluating AI agents on smart contract security auditing.
Given the verified Solidity source code of a real DeFi contract that was later exploited,
can an agent identify the vulnerability before the fact?

## Overview

**Task**: The agent reads contract source code (fetched from Etherscan) and reports
vulnerabilities via a structured `report_finding` tool. No exploit writing. No transaction
simulation. Pure code review.

**Evaluation**: Agent findings are compared to reference findings generated by a
[2+1 LLM council](#how-reference-findings-are-generated) using recall as the primary metric.

**Baseline**: The published baseline uses Azure OpenAI GPT-5.1 with high reasoning effort.

## Dataset

90 real-world DeFi exploits from [DeFiHackLabs](https://github.com/SunWeb3Sec/DeFiHackLabs),
curated and enriched with structured reference findings.

| Chain           | Cases |
|----------------|-------|
| BNB Smart Chain | 44    |
| Ethereum        | 31    |
| Base            | 8     |
| Arbitrum        | 5     |
| Polygon         | 1     |
| Optimism        | 1     |

Total reference findings: **121** (**120** auditable, 1–2 per case)

All cases have `status=ready` and `source_available=true` (verified source on Etherscan).

### How Reference Findings Are Generated

Each case's reference findings were generated by a **2+1 LLM council** using GPT-5.2:

1. **Analyst 1** (full context — Solidity source + DeFiHackLabs PoC):
   Identifies the actual exploited vulnerability. Used as ground truth.

2. **Analyst 2** (code-only — Solidity source, no PoC):
   Reviews the code as an auditor would, without knowing the exploit.
   When Analyst 2 finds the same root cause, its wording is preferred
   because it reads like a natural audit finding.

3. **Judge** (synthesis):
   Combines both. Outputs 1–2 structured findings per case.

The `enrichment_metadata.code_visible` field indicates whether Analyst 2 independently
identified the same root cause as Analyst 1 (i.e. the bug is detectable from code alone).

See [`scripts/README.md`](scripts/README.md) for the full methodology and how to
re-run enrichment on new cases.

## Quick Start

### 1. Clone and install

```bash
git clone https://github.com/Cecuro/defi-vuln-benchmark
cd defi-vuln-benchmark
uv sync
```

### 2. Configure environment

```bash
cp .env.example .env
# Edit .env and fill in your credentials
```

### 3. List available cases

```bash
uv run python cli.py pentest --list
```

### 4. Run a single case (no LLM evaluation)

```bash
uv run python cli.py pentest --case aizpttoken --no-eval
```

### 5. Run a case with full evaluation

```bash
uv run python cli.py pentest --case aizpttoken
```

## Environment Variables

| Variable | Required | Default | Description |
|----------|----------|---------|-------------|
| `AZURE_OPENAI_ENDPOINT` | Yes | — | Your Azure OpenAI endpoint URL |
| `AZURE_OPENAI_API_KEY` | Yes | — | Azure OpenAI API key |
| `AZURE_OPENAI_DEPLOYMENT` | No | `gpt-5.1` | Model deployment name |
| `AZURE_OPENAI_API_VERSION` | No | `2025-03-01-preview` | API version |
| `ETHERSCAN_API_KEY` | Yes* | — | Etherscan API key (*not needed if cache warm) |
| `ETH_RPC_URL` | For ETH cases | — | Ethereum mainnet RPC (31 cases) |
| `BSC_RPC_URL` | For BSC cases | — | BNB Smart Chain RPC (44 cases) |
| `BASE_RPC_URL` | For Base cases | — | Base RPC (8 cases) |
| `ARBITRUM_RPC_URL` | For Arb cases | — | Arbitrum One RPC (5 cases) |
| `POLYGON_RPC_URL` | Optional | — | Polygon RPC (1 case) |
| `OPTIMISM_RPC_URL` | Optional | — | Optimism RPC (1 case) |

> **Tip**: If `.cache/etherscan/` is pre-populated (e.g. committed to the repo),
> `ETHERSCAN_API_KEY` is not needed for running the benchmark.
> RPC URLs are only needed if you enable Foundry setup (default) — skip with `--no-foundry`.

## Running the Benchmark

### Single case

```bash
uv run python cli.py pentest --case aizpttoken
uv run python cli.py pentest --case aizpttoken --no-foundry   # faster, no forge available
uv run python cli.py pentest --case aizpttoken --no-eval      # skip LLM judge step
```

### Filtered batch

```bash
# First 10 cases after a date
uv run python cli.py pentest --after 2024-06-01 --limit 10

# Specific cases
uv run python cli.py pentest --cases aizpttoken,firetoken,hydt
```

### Full benchmark

```bash
uv run python cli.py pentest --all --concurrency 2 --output results/baseline_run_1
```

### All options

```
uv run python cli.py pentest --help

  --list                  List available cases
  --case ID               Run single case
  --cases ID1,ID2,...     Run specific cases
  --all                   Run all cases
  --limit N               Max cases to run
  --after YYYY-MM-DD      Only cases after this date
  --version V             Filter by dataset version (e.g. v1)
  --sort {date,amount,id}

  --agent NAME            Agent to use (default: baseline)
  --time-limit MINUTES    Per-case time limit (default: 60)
  --max-iterations N      Agent iteration limit (default: 500)
  --no-foundry            Skip Foundry project setup
  --no-eval               Skip LLM-as-judge evaluation
  --concurrency N         Parallel runs (default: 1)
  --output DIR            Output directory
```

## Output Format

Each run creates a directory with:

```
results/pentest_baseline_20260219_143022/
├── results.jsonl    # one line per case
└── summary.json     # aggregate statistics
```

### Per-case record (`results.jsonl`)

```json
{
  "case_id": "aizpttoken",
  "chain_id": 56,
  "agent": "baseline",
  "execution_time_seconds": 142.3,
  "findings": [
    {
      "title": "AMM reserve read after deposit inflates buy price",
      "severity": "critical",
      "description": "...",
      "location": "contracts/AIZPTToken.sol, buy() function",
      "recommendation": "..."
    }
  ],
  "evaluation": {
    "recall": 1.0,
    "reference_count": 2,
    "matched_count": 2,
    "novel_findings_count": 1,
    "match_details": [...]
  },
  "error": null
}
```

### Summary (`summary.json`)

```json
{
  "agent": "baseline",
  "total_cases": 90,
  "evaluated_cases": 90,
  "avg_recall": 0.61,
  "total_novel_findings": 47
}
```

## Evaluation Methodology

### Recall

For each case, the judge checks every **auditable** reference finding against all agent findings.
A reference finding is "matched" if any agent finding substantially identifies the same root cause.

```
recall = matched_reference_findings / total_auditable_reference_findings
```

**Auditable** findings are those where `auditable=true` in `reference_findings`.
Non-auditable findings (e.g. social engineering, off-chain oracle manipulation)
are excluded from the denominator.

### Novel findings

`novel_findings_count` tracks agent findings that did not match any reference finding.
These may be valid findings not captured by the reference set, or false positives.
They are reported but do not affect recall.

### Judge prompt

The LLM judge receives:
- The reference finding (title, severity, description)
- All agent findings (title, severity, description, location)

It returns: `{"matched": true/false, "matched_finding_title": "...", "reasoning": "..."}`

Matching is based on root cause overlap, not exact wording.

## Baseline Results

> Results using GPT-5.1 (high reasoning effort), 60-minute time limit per case.

| Cases | Recall |
|-------|--------|
| 90    | —      |

*Fill in after running the benchmark.*

## Adding a New Agent

The benchmark is designed for easy extension.

### Step 1: Create your agent directory

```
src/agents/
└── your_agent/
    ├── __init__.py
    ├── agent.py     # implements YourAgent(BaseAgent)
    └── prompts.py   # (optional) system prompt
```

### Step 2: Implement `BaseAgent`

```python
# src/agents/your_agent/agent.py
from src.agents.base import AgentFinding, BaseAgent

class YourAgent(BaseAgent):
    name = "your_agent"

    async def run(self, working_dir, challenge, config) -> list[AgentFinding]:
        # working_dir/contracts/ contains the Solidity source
        # call report_finding (or build your own) to collect AgentFinding objects
        ...
        return findings
```

### Step 3: Register your agent

```python
# src/agents/__init__.py
from .your_agent import YourAgent

AGENTS: dict[str, type[BaseAgent]] = {
    "baseline": BaselineAgent,
    "your_agent": YourAgent,   # ← add this line
}
```

### Step 4: Run it

```bash
uv run python cli.py pentest --agent your_agent --case aizpttoken
```

See `src/agents/baseline/` for a complete reference implementation.

## Extending the Dataset

To add new cases from DeFiHackLabs:

```bash
# 1. Discover new cases (dry run)
uv run python scripts/1_add_cases_from_defihacklabs.py

# 2. Write new cases to data/cases.jsonl
uv run python scripts/1_add_cases_from_defihacklabs.py --write

# 3. Fetch Etherscan source for new cases
uv run python scripts/2_prefetch_sources.py --resume

# 4. Generate reference findings via 2+1 council
uv run python scripts/3_enrich_cases.py --resume
```

See [`scripts/README.md`](scripts/README.md) for full documentation.

## Citation

If you use this benchmark in your research, please cite:

```bibtex
@misc{dvbench2026,
  title  = {DVBench: DeFi Vulnerability Finding Benchmark},
  author = {cecuro},
  year   = {2026},
  url    = {https://github.com/Cecuro/defi-vuln-benchmark}
}
```
